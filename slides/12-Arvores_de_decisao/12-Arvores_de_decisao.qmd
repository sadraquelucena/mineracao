---
title: "√Årvores de Decis√£o"
subtitle: "ESTAT0109 -- Minera√ß√£o de Dados em Estat√≠stica"
author:
  name: | # indica que o campo name ter√° m√∫ltiplas linhas
    Prof. Dr. Sadraque E. F. Lucena<br>
    <span style="font-size:.8em;">sadraquelucena@academico.ufs.br</span>
    <br>
    <a href="http://sadraquelucena.github.io/mineracao" target="_blank" style="font-size:.8em;">http://sadraquelucena.github.io/mineracao</a>

df-print: default  # Como data frames aparecem no documento
                   # op√ß√µes: "default", "kable", "tibble" e "paged"

format:
  revealjs:
    theme: meutema.scss
    width: 1600
    height: 900
    margin: 0.1         # margem em torno do conte√∫do
    min-scale: 0.2       # escala m√≠nima permitida
    max-scale: 2.0       # escala m√°xima permitida
    slide-number: true
    code-line-numbers: false # sem numero das linhas nos code chunks
    subtitle-style: "font-size: 2em; font-weight: bold;"  # Aqui √© onde aumentamos o subt√≠tulo
    logo: imagens/ufs_horizontal_positiva.png
    transition: fade
    transition-speed: fast
    scss: meutema.scss

lang: pt-BR
---


## Objetivo da Aula

- Compreender o funcionamento do algoritmo de √Årvores de Decis√£o.

---

## Introdu√ß√£o

- √Årvores de decis√£o s√£o m√©todos de classifica√ß√£o que usam a estrat√©gia **dividir para conquistar**.
  - O objetivo √© segmentar os dados em subgrupos menores at√© que eles sejam o mais homog√™neos poss√≠vel (pura separa√ß√£o).

---

## Analogia do Territ√≥rio
- Imagine o conjunto de dados como um grande *territ√≥rio inexplorado*.

  - **Dividir (Cerca):** Escolhemos a caracter√≠stica mais informativa para tra√ßar uma "cerca", dividindo o territ√≥rio em peda√ßos menores.
  - **Conquistar (Avaliar):** Em cada peda√ßo, verificamos se o grupo j√° √© uniforme. Se n√£o for, dividimos novamente.
    
    - Exemplo: "*Se horas de estudo > 5 E frequ√™ncia > 80%...*" (Caminhamos para um lado do mapa).
  
  - **Rotular (Concluir):** Quando n√£o vale mais a pena dividir, chegamos aos **n√≥s terminais (folhas)**. Ali, fincamos uma bandeira com a previs√£o final para aquela √°rea.

---

## √Årvores de Decis√£o

- Modelos que utilizam uma estrutura hier√°rquica (fluxograma) para mapear rela√ß√µes entre preditores e resultados.
- Tipos principais:
  - **√Årvore de Classifica√ß√£o:** O resultado √© uma categoria (ex: Sim/N√£o, Risco Alto/Baixo).
  - **√Årvore de Regress√£o:** O resultado √© um valor num√©rico cont√≠nuo (ex: Sal√°rio, Temperatura).

---

## Anatomia da √Årvore

- **N√≥ Raiz (Root Node):** O topo da √°rvore. Representa a popula√ß√£o inteira antes de qualquer divis√£o.
- **N√≥ de Decis√£o (Internal Node):** Pontos onde o algoritmo faz uma pergunta aos dados (ex: "A renda √© maior que 5k?").
  - Cada resposta cria um **Ramo** (Branch) que leva a um novo n√≥.
- **N√≥ Folha (Leaf Node):** Os pontos terminais. Eles n√£o se dividem mais e cont√™m a decis√£o final (a previs√£o ou classifica√ß√£o).



---

## √Årvores de Decis√£o

![Exemplo de uma √°rvore de decis√£o.](imagens/arvore.png){width=60%}

---

## Interpretabilidade e Regras

- As √Årvores de Decis√£o s√£o modelos **"Caixa Branca"**, ou seja, a l√≥gica interna √© transparente e pode ser lida por humanos.
- A √°rvore pode ser convertida diretamente em um conjunto de regras **SE-ENT√ÉO** (IF-THEN rules). Exemplo:
  - **SE** [Renda < 2.000] **E** [Possui Fiador = N√£o]
  - **ENT√ÉO** [Risco = Alto]
- Por que isso √© importante?
  - **Compliance:** Essencial para setores regulados (Bancos, Sa√∫de, Seguros) que exigem justificativa para decis√µes automatizadas.
  - **Comunica√ß√£o:** Facilita explicar o modelo para gestores n√£o t√©cnicos.

- Vejamos agora como o algoritmo constr√≥i essa estrutura.

---

## Constru√ß√£o da √°rvore (particionamento recursivo)

- **Passo 1:** Escolha do N√≥ Raiz

  - Cada atributo √© utilizado para dividir os dados em grupos distintos (geralmente em dois, no CART).
  - O atributo que gera grupos mais homog√™neos da vari√°vel alvo √© escolhido como n√≥ raiz.
  
- **Passo 2:** Cria√ß√£o de N√≥s Filhos

  - Para cada n√≥ filho, subdivida o grupo usando o atributo que gera subgrupos mais homog√™neos. Crie n√≥s filhos para esses subgrupos.
  
- **Passo 3:** Crit√©rio de Parada

  - O n√≥ √© puro, atingiu profundidade m√°xima ou n√∫mero m√≠nimo de amostras.
  
- **Passo 4:** Atribui√ß√£o de R√≥tulo √†s Folhas

  - **Classifica√ß√£o:** Voto majorit√°rio (Moda).
  - **Regress√£o:** M√©dia dos valores do n√≥.

---

## Constru√ß√£o da √°rvore

### Exemplo

Um est√∫dio de Hollywood precisa decidir quais roteiros de autores iniciantes devem entrar em produ√ß√£o. Como n√£o h√° tempo para ler completamente cada roteiro, voc√™ decide usar um algoritmo de √°rvore de decis√£o para prever se um poss√≠vel filme se encaixaria em uma das tr√™s categorias: **Sucesso de Cr√≠tica**, **Sucesso de P√∫blico** ou **Fracasso de Bilheteria**.

- Para construir a √°rvore voc√™ considera como dados de treino os 30 lan√ßamentos recentes do est√∫dio. Rapidamente voc√™ percebe uma rela√ß√£o entre o or√ßamento estimado para as filmagens, o n√∫mero de celebridades escaladas para pap√©is principais e o n√≠vel de sucesso. Animado com essa descoberta, voc√™ produz um gr√°fico de dispers√£o para ilustrar o padr√£o:

---

## Constru√ß√£o da √°rvore
![](imagens/hollywood1.png){width=80%}

---

## Constru√ß√£o da √°rvore

- **Passo 1:** Para criar o n√≥ raiz usamos o n√∫mero de celebridades no filme.

![](imagens/hollywood2.png){width=50%}

---

## Constru√ß√£o da √°rvore

- **Passo 2:** Nos filmes com muitas celebridades, subdividimos entre alto e baixo or√ßamento.

![](imagens/hollywood3.png){width=50%}

---

## Constru√ß√£o da √°rvore

- **O Risco do Crescimento Ilimitado:**
  - Poder√≠amos dividir os dados at√© que cada folha fosse 100% pura (apenas uma observa√ß√£o por folha).
  - Isso levaria ao **Sobreajuste (Overfitting)**: a √°rvore "decora" o ru√≠do dos dados de treinamento e perde a capacidade de **Generaliza√ß√£o**.
- **Equil√≠brio entre Complexidade e Erro:**
  - √Årvores muito grandes s√£o complexas e inst√°veis.
  - √Årvores muito pequenas podem ser simplistas (Underfitting).
- **Nosso Crit√©rio de Parada (Heur√≠stica):**
  - Interrompemos a divis√£o quando um n√≥ atinge **80% de homogeneidade** (uma classe √© amplamente majorit√°ria).


---

## Constru√ß√£o da √°rvore

![](imagens/hollywood-arvore.png){width=55%}

---

## Como o algoritmo prop√µe as divis√µes?

Antes de escolher o melhor atributo, o algoritmo identifica todos os pontos de divis√£o poss√≠veis:

- **Para Vari√°veis Categ√≥ricas:** Testa todas as combina√ß√µes bin√°rias de grupos.
  - **Exemplo:** Atributo Cor {üî¥, üîµ, üü¢}
  
    - {üî¥} vs {üîµ, üü¢}
    - {üîµ} vs {üî¥, üü¢}
    - {üü¢} vs {üî¥, üîµ}

- **Para Vari√°veis Num√©ricas:** Os dados s√£o ordenados e os candidatos s√£o os **pontos m√©dios** entre valores consecutivos observados.
  - **Exemplo:** Valores de Or√ßamento {1, 3, 8, 11}
    - Candidatos: $\le 2.0$ | $\le 5.5$ | $\le 9.5$

---

## Crit√©rio de Sele√ß√£o: Pureza e Impureza

- O algoritmo busca a **melhor separa√ß√£o**. Para isso, ele precisa medir qu√£o "misturados" os dados ficaram ap√≥s o corte.
- **N√≥ Puro:** Cont√©m apenas observa√ß√µes de uma √∫nica classe.
  - **Incerteza zero**. Se cair aqui, a previs√£o √© √≥bvia.
- **N√≥ Impuro:** Cont√©m uma mistura de classes.
  - O caso de m√°xima impureza ocorre quando as classes est√£o distribu√≠das igualmente (ex: 50% Sucesso / 50% Fracasso).

- **A Regra de Ouro:** O algoritmo testa todos os candidatos a corte (vistos no slide anterior) e escolhe aquele que resulta na menor impureza m√©dia nos n√≥s filhos.

---

## Entropia (Defini√ß√£o)

- A **Entropia** quantifica o n√≠vel de desordem ou impureza de uma parti√ß√£o (quanto maior a "mistura", maior a entropia).
- Para um conjunto $D$ com $C$ classes, a entropia √© calculada como:
$$
\text{Entropia}(D) = - \sum_{c=1}^C p_c  \log_2(p_c),
$$
em que

- $c$ representa uma classe da sa√≠da ($c=1,\ldots,C$);
- $p_i$ √© a propor√ß√£o (probabilidade) de dados pertencentes a classe $c$;
- $\log_2(\cdot)$ √© o lograritmo na base 2.

---

## Entropia (Propriedades)

- **Escala:** Varia de $0$ (Pureza total: todos os dados s√£o da mesma classe) a $\log_2(C)$ (Impureza m√°xima: todas as classes em propor√ß√µes iguais).
- **Caso Bin√°rio ($C=2$):** A entropia varia entre $0$ e $1$.
  - Se $p_1 = 1$ e $p_2 = 0$, ent√£o $\text{Entropia} = 0$ (N√≥ puro).
  - Se $p_1 = 0,5$ e $p_2 = 0,5$, ent√£o $\text{Entropia} = 1$ (M√°xima incerteza).

![](imagens/entropia.png){fig-align="center"}

## Exemplo 12.1

Considere um conjunto de dados de treinamento com informa√ß√µes de 30 clientes de um banco, onde 16 quitaram suas d√≠vidas (adimplentes) e 14 n√£o quitaram (inadimplentes). Para analisar a entropia, uma parti√ß√£o √© criada com base na decis√£o de se o cliente solicitou um empr√©stimo maior ou menor que R$ 50.000.

a. Dentre os clientes que solicitaram mais de R$ 50.000, 10 s√£o adimplentes e 6 s√£o inadimplentes.  Calcule a entropia (grau de impureza) nessa parti√ß√£o.
$$
\text{Entropia}(D) = -\left[ \frac{10}{16}\,\log_2\left(\frac{10}{16}\right) + \frac{6}{16}\,\log_2\left(\frac{6}{16} \right) \right] = 0,\!9544
$$

- O valor est√° muito pr√≥ximo de 1 (m√°xima impureza para 2 classes).
- Isso significa que a vari√°vel "Empr√©stimo > 50k", isoladamente, ainda deixa os grupos muito misturados. Precisamos de mais divis√µes!


## Entropia

- Podemos visualizar como a entropia varia em rela√ß√£o √† distribui√ß√£o dos exemplos quando temos duas classes.
- Ao conhecer a propor√ß√£o de exemplos em uma classe ($p_c$), automaticamente sabemos que a propor√ß√£o na outra classe √© ($1-p_c$).
- Ent√£o temos:

```{r, fig.height=4, fig.width=6.5} 
#| echo: false
#| warning: false
#| error: false
#| fig-align: center

# Carregue o pacote ggplot2
library(ggplot2)

# Crie um data frame com valores de x e a entropia correspondente
data <- data.frame(x = seq(0.01, 0.99, by = 0.01),
                   entropy = -seq(0.01, 0.99, by = 0.01) * log2(seq(0.01, 0.99, by = 0.01))
                             - (1 - seq(0.01, 0.99, by = 0.01)) * log2(1 - seq(0.01, 0.99, by = 0.01)))

ggplot(data, aes(x = x, y = entropy)) +
  geom_line(color = "firebrick", linewidth = 1.5) + # 'linewidth' √© o padr√£o atual
  geom_vline(xintercept = 0.5, linetype = "dashed", color = "gray50") + # Linha no centro
  annotate("text", x = 0.5, y = 1.05, label = "M√°xima Impureza", size = 6) +
  labs(x = expression(p[c]), y = "Entropia") +
  scale_x_continuous(breaks = seq(0, 1, 0.1)) +
  theme_minimal() +
  theme(axis.text = element_text(size = 18), 
        axis.title = element_text(size = 20))
```

## Ganho de Informa√ß√£o

- **O que √©:** Uma m√©trica para selecionar qual atributo deve formar o pr√≥ximo n√≥ da √°rvore.
- **L√≥gica:** Mede a **redu√ß√£o da incerteza**. O melhor atributo √© aquele que mais "limpa" a confus√£o dos dados.
- **C√°lculo:** √â a diferen√ßa entre a entropia antes da divis√£o (Pai) e a entropia ap√≥s a divis√£o (Filhos).
- F√≥rmula:
$$
\text{Ganho de Informa√ß√£o}(A) = \text{Entropia}(D_{pai}) - \text{Entropia}(D_{filhos}).
$$
- **Objetivo:** Maximizar o Ganho de Informa√ß√£o.
- **Resultado:** Ramos mais homog√™neos (puros) e uma √°rvore mais eficiente.

---

## Refinando o Ganho: Pesos e Propor√ß√µes

### 1. Entropia Combinada dos Filhos
- Como uma divis√£o gera v√°rios n√≥s, calculamos a m√©dia ponderada das entropias:
$$
\text{Entropia}(D_{filhos}) = \sum_{i=1}^n w_i \,\text{Entropia}(D_i)
$$

- $w_i$: √â o "peso" do n√≥ (propor√ß√£o de dados que foi para aquele ramo).
  - Ex: Se um ramo recebeu 20 de 30 clientes, seu peso $w = 20/30$.

---

## Refinando o Ganho: Pesos e Propor√ß√µes

### 2. A Armadilha do Ganho de Informa√ß√£o

- **Problema:** O Ganho simples tende a favorecer atributos com muitas categorias √∫nicas (ex: ID, CPF ou Datas). Eles criam parti√ß√µes "perfeitas", mas in√∫teis para prever novos dados.
- **Solu√ß√£o (Quinlan, 1986):** Usar a Taxa de Ganho (Gain Ratio), que penaliza divis√µes excessivas:
  $$
    \text{Taxa de Ganho}(A) = \frac{\text{Ganho de Informa√ß√£o}(A)}{\text{Informa√ß√£o Intr√≠nseca}(A)}
  $$
  - Mais informa√ß√µes sobre a taxa de ganho podem ser encontradas em *Quinlan, J. Ross. ‚ÄúInduction of Decision Trees.‚Äù Machine Learning 1.1 (1986): 81‚Äì106.*

---

## Exemplo 12.2

Vamos prosseguir com os dados do Exemplo 12.1. Agora, estamos prestes a criar uma parti√ß√£o no n√≥ correspondente aos clientes que solicitaram mais de R\$ 50.000. Nesse ponto, enfrentamos a escolha entre dois atributos para esta nova divis√£o: o *grau do empr√©stimo* (com categorias $A$ e $B$) e *posse do im√≥vel* (pr√≥prio ou alugado). Para determinar qual atributo ser√° utilizado na nova parti√ß√£o, √© essencial avaliar o ganho de informa√ß√£o associado a cada um deles. Calcule o ganho de informa√ß√£o baseado na imagem abaixo.

![](imagens/ganho-informacao.png){width="60%"}

---

## √çndice de Gini: Alternativa Pr√°tica

- **O que √©:** A m√©trica padr√£o do algoritmo CART. Mede a probabilidade de um elemento ser classificado incorretamente se rotularmos aleatoriamente.
- **Vantagem:** √â computacionalmente mais r√°pido que a Entropia (evita o c√°lculo de $\log$).
- F√≥rmula:
$$
\text{Gini}(D) = 1 - \sum_{c=1}^C p_c^2
$$
em que $p_c$ √© a propor√ß√£o de dados da classe $c$.

- O √≠ndice de Gini varia de $0$ a $(C-1)/C$.
- Para duas classes ($C=2$), o √≠ndice de Gini varia de $0$ a $0,\!5$.
  - **M√≠nimo (0.0):** Pureza total (ex: 100% adimplentes $\rightarrow 1 - 1^2 = 0$).
  - **M√°ximo (0.5):** Impureza m√°xima (50/50 $\rightarrow 1 - (0,5^2 + 0,5^2) = 0,5$).

---

## Poda da √Årvore (Pruning)

- **Problema:** Uma √°rvore que cresce at√© a pureza total tende a sofrer de **Overfitting**. Ela "decora" o ru√≠do dos dados de treino e perde o poder de **generaliza√ß√£o**.
- **Solu√ß√£o:** Reduzir a complexidade da √°rvore atrav√©s da poda.
- **Objetivo:** Encontrar o "ponto ideal" onde a √°rvore √© simples o suficiente para ser interpret√°vel, mas complexa o suficiente para capturar os padr√µes reais.

- Dois tipos de poda podem ser usados: **pr√©-poda** **p√≥s-poda**.

---

## Pr√©-poda (Early Stopping)

- Ocorre **durante** a constru√ß√£o. O algoritmo decide n√£o dividir um n√≥ se certas condi√ß√µes forem atendidas:
  - **Profundidade M√°xima:** Ex: "N√£o passe de 5 n√≠veis".
  - **M√≠nimo de Amostras por N√≥:** Ex: "N√£o divida n√≥s com menos de 20 clientes".
  - **Ganho M√≠nimo:** Ex: "S√≥ divida se o Ganho de Informa√ß√£o for $> 0.05$".

- **Vantagem:** Muito r√°pida e eficiente.
- **Desvantagem:** Pode ser simplista demais (Underfitting).

---

## P√≥s-poda (Post-pruning)

- Deixa a √°rvore atingir sua **complexidade m√°xima** e depois "corta" os ramos que contribuem pouco para a acur√°cia.
- **Estrat√©gia:** Substitui um sub-ramo inteiro por um √∫nico n√≥ folha.
- **Vantagem:** Mais robusta. Ela permite que o algoritmo descubra rela√ß√µes complexas que s√≥ aparecem ap√≥s v√°rias divis√µes.
- **M√©trica comum:** Cost-Complexity Pruning (usada no algoritmo CART).

---

## Exemplo 12.3

Considere os dados abaixo e construa uma √°rvore de decis√£o para a vari√°vel *sair* usando a entropia para quantificar a impureza.

| Tempo       | Temp. | FDS | Jogar? |Tempo       | Temp. | FDS | Jogar? |
|:----|:----|:----|:----|:----|:----|:----|:----|
| Ensolarado  | Quente      | Sim           | Sim             | Ensolarado  | Quente      | N√£o           | N√£o             |
| Chuvoso     | Moderado    | N√£o           | N√£o             | Chuvoso     | Moderado    | Sim           | Sim             |
| Chuvoso     | Quente      | Sim           | N√£o             | Ensolarado  | Quente      | Sim           | Sim             |
| Ensolarado  | Moderado    | N√£o           | Sim             | Chuvoso     | Moderado    | N√£o           | N√£o             |
| Chuvoso     | Quente      | N√£o           | N√£o             | Ensolarado  | Quente      | Sim           | Sim             |
| Ensolarado  | Moderado    | Sim           | Sim             | Chuvoso     | Moderado    | N√£o           | N√£o             |
| Chuvoso     | Quente      | Sim           | Sim             | Ensolarado  | Quente      | Sim           | Sim             |

<br>

FDS: fim de semana.$\quad$Temp.: temperatura.

---

## Algoritmos

- Existem diversos algoritmos de √°rvores de decis√£o, cada um com suas caracter√≠sticas espec√≠ficas e m√©todos de constru√ß√£o. Alguns s√£o mais adequados para determinados tipos de problemas ou conjuntos de dados do que outros. 
- Nesta aula veremos os algoritmos:

    - CART (*Classification and Regression Trees*)
    - C5.0
    
- Outros algoritmos podem ser encontrados em [https://topepo.github.io/caret/available-models.html](https://topepo.github.io/caret/available-models.html)

---

## Algoritmo CART (Classification And Regression Tree)

- **Origem:** Proposto por Breiman, Friedman, Olshen e Stone (1984). √â o "padr√£o ouro" das √°rvores modernas.
- **Estrutura Estritamente Bin√°ria:** Diferente de outros algoritmos, o CART **sempre** divide um n√≥ em exatamente dois filhos (Esquerda/Direita).
- **Crit√©rios de Divis√£o (Splitting Criteria):**

  - **√Årvores de Classifica√ß√£o (Alvo Categ√≥rico):** Busca minimizar a impureza usando o **√çndice de Gini**.
    
  - **√Årvores de Regress√£o (Alvo Num√©rico):** Busca a **Redu√ß√£o da Vari√¢ncia** (minimiza a Soma dos Quadrados dos Res√≠duos - RSS) dentro de cada n√≥.

---

## Algoritmo CART: Vantagens

- **Robustez a Dados Ausentes (Surrogate Splits):** Grande diferencial do CART. Se um dado falta, ele usa "vari√°veis substitutas" (altamente correlacionadas com a principal) para decidir o caminho, sem perder a observa√ß√£o.
- **Interpretabilidade (Caixa Branca):** Gera regras l√≥gicas claras e requer pouco pr√©-processamento (n√£o exige normaliza√ß√£o dos dados).
- **Sele√ß√£o Impl√≠cita de Atributos:** Vari√°veis irrelevantes simplesmente n√£o s√£o selecionadas para os cortes.
- **Resili√™ncia a Outliers (nos preditores):** Como a divis√£o √© baseada em rank (ordena√ß√£o) e n√£o em dist√¢ncia, outliers nas vari√°veis explicativas isoladas t√™m pouco impacto na estrutura.

---

## Algoritmo CART: Desvantagens

- **Alta Vari√¢ncia (Instabilidade):** √â sens√≠vel aos dados de treino. Uma pequena altera√ß√£o na amostra pode gerar uma √°rvore completamente diferente.
  - *Solu√ß√£o:* Utilizar m√©todos de Ensemble (ex: Random Forest).
- **Tend√™ncia ao Sobreajuste (Overfitting):** Sem poda (pruning), a √°rvore cresce at√© decorar o ru√≠do dos dados.
- **Vi√©s de Cardinalidade:** Tende a favorecer vari√°veis com muitas categorias/n√≠veis distintos, caso n√£o sejam tratados.
- **Fronteiras Ortogonais:** As divis√µes s√£o sempre retangulares (paralelas aos eixos), o que dificulta capturar rela√ß√µes diagonais complexas.

---

## Algoritmo C5.0

- Criado por Ross Quinlan. √â a vers√£o comercial e otimizada do C4.5 (sucessor do ID3).
- Projetado especificamente para problemas de Classifica√ß√£o.
- **Estrutura de Divis√£o (Multi-way Splits):** Diferente do CART (bin√°rio), o C5.0 pode gerar m√∫ltiplos ramos em um √∫nico n√≥ para vari√°veis categ√≥ricas (ex: um ramo para cada UF).
- **Crit√©rio de Sele√ß√£o:** Utiliza a **Taxa de Ganho (Gain Ratio)** para reduzir o vi√©s da Entropia em dire√ß√£o a atributos com muitas categorias.

---

## Algoritmo C5.0: Vantagens

- **Performance Superior:** Extremamente r√°pido e eficiente em mem√≥ria (significativamente melhor que o C4.5).
- **Suporte Nativo a Boosting:** Permite criar um ensemble de √°rvores para aumentar a precis√£o de previs√µes dif√≠ceis.
- **Winnowing (Sele√ß√£o de Atributos):** Possui um mecanismo avan√ßado para "limpar" preditores in√∫teis antes de construir a √°rvore.
- **Poda Global (Global Pruning):** Realiza uma poda p√≥s-constru√ß√£o baseada em intervalos de confian√ßa binomiais, tornando o modelo final mais generalista.

---

### Algoritmo C5.0: Desvantagens

- N√£o realiza Regress√£o (vari√°vel resposta deve ser sempre categ√≥rica).
- **"√Årvores "Largas":** Devido √†s divis√µes m√∫ltiplas (multi-way splits), a √°rvore pode ficar muito larga e dif√≠cil de visualizar em compara√ß√£o √†s √°rvores bin√°rias do CART.
- **Complexidade de Interpreta√ß√£o com Boosting:** Se a fun√ß√£o de Boosting for ativada, a interpretabilidade (regras simples) √© sacrificada em troca de acur√°cia, aproximando-se de uma "caixa preta".





# Agora vamos fazer no R...